from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    TimeoutException, 
    NoSuchElementException, 
    WebDriverException
)
from bs4 import BeautifulSoup
from urllib.parse import quote_plus
from webdriver import setup_driver

import time
import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Optional, Tuple


# --- LOGGING SETUP ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# --- CONFIGURATION ---
class ScraperConfig:
    """Configuration settings for the LinkedIn scraper"""
    
    # Scraping parameters
    COUNTRIES = ["Egypt"]
    DATE_POSTED = "any"  # "any", "24h", "week", "month"
    WORKPLACE_TYPES = []  # ["1"]=On-site, ["2"]=Remote, ["3"]=Hybrid
    EXPERIENCE_LEVELS = []  # ["1"]=Internship, ["2"]=Entry, ["3"]=Associate, etc.
    
    # Scrolling and timing
    MAX_SCROLL_ATTEMPTS = 200
    SCROLL_PAUSE = 5
    DETAIL_PAUSE = 2
    RETRY_ATTEMPTS = 3
    RETRY_DELAY = 3
    
    # Output directory
    OUTPUT_DIR = "Data/Collected"
    
    # Browser options
    HEADLESS = False
    INCOGNITO = True


# --- HELPER FUNCTIONS ---

def sanitize_filename(filename: str) -> str:
    """
    Sanitize filename by removing invalid characters
    
    Args:
        filename: Original filename
        
    Returns:
        Sanitized filename safe for filesystem
    """
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    return filename.strip()


def build_linkedin_url(
    keyword: str,
    location: str,
    exp_levels: List[str],
    workplace_types: List[str],
    date_posted: str
) -> str:
    """
    Build LinkedIn job search URL with filters
    
    Args:
        keyword: Job title/keyword to search
        location: Location to search in
        exp_levels: Experience level filters
        workplace_types: Workplace type filters
        date_posted: Date posted filter
        
    Returns:
        Formatted LinkedIn search URL
    """
    exp_param = ",".join(exp_levels) if exp_levels else ""
    workplace_param = ",".join(workplace_types) if workplace_types else ""
    
    date_param = ""
    date_map = {
        "24h": "r86400",
        "week": "r604800",
        "month": "r2592000"
    }
    date_param = date_map.get(date_posted, "")

    url = f"https://www.linkedin.com/jobs/search/?keywords={quote_plus(keyword)}&location={quote_plus(location)}"
    
    if exp_param:
        url += f"&f_E={exp_param}"
    if workplace_param:
        url += f"&f_WT={workplace_param}"
    if date_param:
        url += f"&f_TPR={date_param}"
    
    url += "&position=1&pageNum=0"
    return url


def scroll_page(driver, config: ScraperConfig) -> bool:
    """
    Scroll through LinkedIn job listings to load all results
    
    Args:
        driver: Selenium WebDriver instance
        config: Scraper configuration
        
    Returns:
        True if scrolling completed successfully, False otherwise
    """
    try:
        attempt = 0
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        logger.info("Starting page scroll to load all job listings...")
        
        while attempt < config.MAX_SCROLL_ATTEMPTS:
            # Scroll to bottom
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(config.SCROLL_PAUSE)
            
            # Try to click "Show more" button if present
            try:
                show_more_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "infinite-scroller__show-more-button"))
                )
                show_more_btn.click()
                logger.debug("Clicked 'Show more' button")
                time.sleep(config.SCROLL_PAUSE)
            except TimeoutException:
                pass  # Button not present, continue scrolling
            except Exception as e:
                logger.debug(f"Error clicking 'Show more': {str(e)}")
            
            # Check if we've reached the bottom
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                logger.info("Reached bottom of page")
                break
                
            last_height = new_height
            attempt += 1
            
        return True
        
    except Exception as e:
        logger.error(f"Error during page scrolling: {str(e)}")
        return False


def fetch_job_details(driver, job_url: str, config: ScraperConfig) -> Tuple[str, str]:
    """
    Fetch detailed job description and company info from job page
    
    Args:
        driver: Selenium WebDriver instance
        job_url: URL of the job posting
        config: Scraper configuration
        
    Returns:
        Tuple of (job_description, company_description)
    """
    job_desc = ""
    company_desc = ""
    
    if not job_url:
        return job_desc, company_desc
    
    for attempt in range(config.RETRY_ATTEMPTS):
        try:
            driver.get(job_url)
            time.sleep(config.DETAIL_PAUSE)
            
            # Wait for job description to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "description__text"))
            )
            
            job_soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # Extract job description
            job_div = job_soup.find("div", class_="description__text")
            if job_div:
                job_desc = job_div.get_text(separator="\n", strip=True)
            
            # Extract company description
            company_div = job_soup.find("div", class_="show-more-less-html__markup")
            if company_div:
                company_desc = company_div.get_text(separator="\n", strip=True)
            
            return job_desc, company_desc
            
        except TimeoutException:
            logger.warning(f"Timeout loading job details (attempt {attempt + 1}/{config.RETRY_ATTEMPTS})")
            if attempt < config.RETRY_ATTEMPTS - 1:
                time.sleep(config.RETRY_DELAY)
            else:
                logger.error(f"Failed to load job details after {config.RETRY_ATTEMPTS} attempts: {job_url}")
                
        except Exception as e:
            logger.error(f"Error fetching job details (attempt {attempt + 1}): {str(e)}")
            if attempt < config.RETRY_ATTEMPTS - 1:
                time.sleep(config.RETRY_DELAY)
    
    return job_desc, company_desc


def extract_job_cards(html: str) -> List[BeautifulSoup]:
    """
    Extract job cards from page HTML
    
    Args:
        html: Page HTML source
        
    Returns:
        List of BeautifulSoup job card elements
    """
    try:
        soup = BeautifulSoup(html, "html.parser")
        job_cards = soup.find_all("div", class_="base-card")
        logger.info(f"Found {len(job_cards)} job cards on page")
        return job_cards
    except Exception as e:
        logger.error(f"Error extracting job cards: {str(e)}")
        return []


def parse_job_card(card: BeautifulSoup, country: str) -> Dict:
    """
    Parse individual job card to extract job information
    
    Args:
        card: BeautifulSoup job card element
        country: Country being scraped
        
    Returns:
        Dictionary with job information
    """
    try:
        # Job title and URL
        a_tag = card.find("a", class_="base-card__full-link")
        job_url = a_tag["href"].strip() if a_tag else ""
        
        sr_only = a_tag.find("span", class_="sr-only") if a_tag else None
        job_title = sr_only.text.strip() if sr_only else ""
        
        # Company info
        company_tag = card.find("h4", class_="base-search-card__subtitle")
        company_a = company_tag.find("a") if company_tag else None
        company_name = company_a.text.strip() if company_a else ""
        company_url = company_a["href"].strip() if company_a else ""
        
        # Location
        location_tag = card.find("span", class_="job-search-card__location")
        location = location_tag.text.strip() if location_tag else ""
        
        # Benefits
        benefit_tag = card.find("span", class_="job-posting-benefits__text")
        benefit = benefit_tag.text.strip() if benefit_tag else ""
        
        # Posted date
        posted_tag = card.find("time", class_="job-search-card__listdate")
        posted = posted_tag.text.strip() if posted_tag else ""
        
        return {
            "country": country,
            "job_title": job_title,
            "company_name": company_name,
            "company_url": company_url,
            "location": location,
            "benefit": benefit,
            "posted": posted,
            "job_url": job_url
        }
        
    except Exception as e:
        logger.error(f"Error parsing job card: {str(e)}")
        return None


def save_job_to_json(job_data: Dict, domain: str, config: ScraperConfig) -> bool:
    """
    Save job data to JSON file in organized directory structure
    
    Args:
        job_data: Dictionary containing job information
        domain: Job domain/category
        config: Scraper configuration
        
    Returns:
        True if saved successfully, False otherwise
    """
    try:
        # Create domain directory
        domain_dir = os.path.join(config.OUTPUT_DIR, sanitize_filename(domain))
        os.makedirs(domain_dir, exist_ok=True)
        
        # Create filename: {job_title}-{company}.json
        job_title = sanitize_filename(job_data.get("job_title", "unknown"))
        company = sanitize_filename(job_data.get("company_name", "unknown"))
        filename = f"{job_title}-{company}.json"
        filepath = os.path.join(domain_dir, filename)
        
        # Add metadata
        job_data["scraped_at"] = datetime.now().isoformat()
        job_data["domain"] = domain
        
        # Save to JSON
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(job_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Saved: {filepath}")
        return True
        
    except Exception as e:
        logger.error(f"Error saving job to JSON: {str(e)}")
        return False


def scrape_jobs(
    job_keyword: str,
    domain: str,
    config: ScraperConfig,
    driver=None
) -> List[Dict]:
    """
    Main scraping function for a specific job keyword
    
    Args:
        job_keyword: Job title/keyword to search
        domain: Job domain/category
        config: Scraper configuration
        driver: Optional existing WebDriver instance
        
    Returns:
        List of scraped job dictionaries
    """
    close_driver = False
    if driver is None:
        driver = setup_driver()
        close_driver = True
    
    all_jobs = []
    
    try:
        for country in config.COUNTRIES:
            logger.info(f"=== Scraping LinkedIn Jobs: '{job_keyword}' in {country} ===")
            
            # Build search URL
            url = build_linkedin_url(
                job_keyword,
                country,
                config.EXPERIENCE_LEVELS,
                config.WORKPLACE_TYPES,
                config.DATE_POSTED
            )
            logger.info(f"üîó URL: {url}")
            
            # Load search results page
            try:
                driver.get(url)
                time.sleep(3)  # Wait for initial load
            except WebDriverException as e:
                logger.error(f"Failed to load search page: {str(e)}")
                continue
            
            # Scroll to load all results
            if not scroll_page(driver, config):
                logger.warning("Scrolling failed, continuing with loaded results...")
            
            # Extract job cards
            html = driver.page_source
            job_cards = extract_job_cards(html)
            
            if not job_cards:
                logger.warning(f"No jobs found for '{job_keyword}' in {country}")
                continue
            
            # Process each job card
            for idx, card in enumerate(job_cards, 1):
                logger.info(f"üîç Processing job {idx}/{len(job_cards)}")
                
                # Parse basic job info
                job_data = parse_job_card(card, country)
                if not job_data:
                    continue
                
                logger.info(f"  üìã {job_data['job_title']} @ {job_data['company_name']}")
                
                # Fetch detailed job description
                job_desc, company_desc = fetch_job_details(
                    driver,
                    job_data["job_url"],
                    config
                )
                
                job_data["job_description"] = job_desc
                job_data["company_description"] = company_desc
                
                # Save to JSON
                save_job_to_json(job_data, domain, config)
                
                all_jobs.append(job_data)
        
        logger.info(f"‚úÖ Completed scraping: {len(all_jobs)} jobs collected")
        return all_jobs
        
    except Exception as e:
        logger.error(f"Unexpected error during scraping: {str(e)}")
        return all_jobs
        
    finally:
        if close_driver and driver:
            driver.quit()


def main():
    """Main entry point for the scraper"""
    config = ScraperConfig()
    
    # Example usage - scrape a single job title
    job_keyword = "Software Engineer"
    domain = "Software Engineering"
    
    logger.info("üöÄ Starting LinkedIn Job Scraper")
    logger.info(f"üìÇ Output directory: {config.OUTPUT_DIR}")
    
    # Create output directory
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    # Run scraper
    jobs = scrape_jobs(job_keyword, domain, config)
    
    logger.info(f"üéâ Scraping completed! Total jobs: {len(jobs)}")


if __name__ == "__main__":
    main()
